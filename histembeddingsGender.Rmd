---
title: "Tran_Riddle_Historical Embeddings"
output: 
  html_document:
    toc: true
    toc_float: true
  github_document:
date: December 3, 2024
author: Nela Riddle
---
[Visit the original repository on GitHub](https://github.com/nelariddle/histembeddingsgender)

We load in the required packages.

```{r loadPackages, echo = FALSE, message=FALSE, warning=FALSE}
########################################################

## What are the historical patterns of social group representations?
## 14 social groups, 200 years of Google Books Text

########################################################

## Set up R workspace and packages ----
## Set WD to source file location (python codes, stored data, scripts, etc. can all be shared to one folder for ease of use)
# setwd("**")

# load packages
if (!require("corrplot")) {install.packages("corrplot", dependencies = TRUE);require(corrplot)}
if (!require("reticulate")) {install.packages("reticulate", dependencies = TRUE);require(reticulate)}
if (!require("lsa")) {install.packages("lsa", dependencies = TRUE);require(lsa)}
if (!require("dplyr")) {install.packages("dplyr", dependencies = TRUE);require(dplyr)}
if (!require("devtools")) {install.packages("devtools", dependencies = TRUE);require(devtools)}
# if (!require("sweater")) {devtools::install_github("chainsawriot/sweater");require(sweater)} 
if (!require("sweater")) {install.packages("sweater");require(sweater)} 
if (!require("writexl")) {install.packages("writexl");require(writexl)} 
# contains convenience functions for embeddings analyses 
# see: https://rdrr.io/github/chainsawriot/sweater/f/README.md
```

### Setup
First, load in the pre-written group and word lists to be used in analyses. We are creating a custom function for reading in the word lists. It assumes no headers and single vector of words.

Then, we pull in the group words and trait/role words files file and doing some basic cleaning.

```{r loadWords, include=TRUE, warning=FALSE}
## Load in data ----
## Set WD to word stimuli
setwd("wordstim")

# Function to read and process lists
read_list <- function(file, col_name) {
  list_data <- read.delim(file, header = FALSE)
  colnames(list_data) <- col_name
  return(as.vector(list_data[[col_name]]))
}

# Specific groups (men, women)
groupwrds <- read.csv("groupstimlists.csv", header = FALSE)
groupwrds <- as.data.frame(t(groupwrds))
colnames(groupwrds) <- as.character(groupwrds[1, ])
groupwrds <- groupwrds[-1, ]

# Read lists using the function
agentic <- read_list("agentic.txt", "agentic")
communal <- read_list("communal.txt", "communal")
trait <- read_list("traitlist.txt", "trait")
chore <- read_list("chore.txt", "chore")
role <- read_list("joblistDOT.txt", "role")
role2 <-
  sub("s$", "", tolower(
    read.csv(
      "All_Occupations.csv",
      header = TRUE,
      stringsAsFactors = FALSE
    )$Occupation[!grepl(
      "\\s",
      read.csv(
        "All_Occupations.csv",
        header = TRUE,
        stringsAsFactors = FALSE
      )$Occupation
    )]
  ))
role <- union(role, role2)
role <- union(role, chore)


noun <- read_list("nouns.txt", "noun")
common <- read_list("common.txt", "common")

setwd("..")
```
The agentic and communal lists were borrowed from https://onlinelibrary.wiley.com/doi/10.1002/ejsp.2561; here are some examples:
```{r wordExamples}
head(agentic)
head(communal)
```

The group word lists were taken from https://pubmed.ncbi.nlm.nih.gov/35787033/, as well as the trait list. The choice was made to remove "bride" because the male equivalent is ambiguous, as well as "miss." "Womanly" was not in the original source code but was added.x
```{r groupExamples}
head(groupwrds$men)
head(groupwrds$women)
head(trait)
```
The role titles were scraped off this site: https://theodora.com/dot_index.html, a 1971 survey on role titles. They were merged with one-word titles from ONET, the modern equivalent: https://www.onetonline.org/find/all. They were merged with the chore list to represent unpaid labor.
```{r roleExamples, echo=FALSE, message=FALSE}
head(role)
```

Now, we define which words are not available in both sets of embeddings across time.

This was a decision made by the original folks who trained these embeddings (histwords) "Furthermore, the authors only computed embeddings for words that were among the top 100,000 most frequent words over all time (for EngAll) or top 50,000 most frequent words (for COHA), and further discarded any words that occurred less than 500 times (for EngAll) or 100 times (for COHA) in a given decade."

```{r loadVectors, echo=FALSE, message=FALSE}
# Check unavailable words by decade ----
if (!exists("wordvecs.dat", envir = .GlobalEnv)) {
  load("engall/wordvecsdata_engall.RData", envir = .GlobalEnv)
}
if (!exists("wordvecs.dat_coha", envir = .GlobalEnv)) {
  load("coha/wordvecsdata_coha.RData", envir = .GlobalEnv)
}
unavwords <- lapply(wordvecs.dat, function(x)
  rownames(x)[which(x$V1 == 0)])
n_avwords <- sapply(wordvecs.dat, function(x)
  length(x$V1) - length(rownames(x)[which(x$V1 == 0)]))

unavwords_coha <- lapply(wordvecs.dat_coha, function(x)
  rownames(x)[which(x$V1 == 0)])
n_avwords_coha <- sapply(wordvecs.dat_coha, function(x)
  length(x$V1) - length(rownames(x)[which(x$V1 == 0)])
)
```
### Producing the mac scores

The workhorse function, which iterates over each decade, computing the MAC (mean average cosine similarity) score between each word and each group, then finds the Pearson correlation of the resulting lists (demonstrated visually later).

```{r importantFunction, message=FALSE}
grpwrdassoc_rel <-
  function(group1index,
           group2index,
           wordterms,
           wordvecs.dat = wordvecs.dat,
           unavwords = unavwords,
           corpus,
           parallelize = FALSE) {
    
    start_year <- if (corpus == "coha")
      1820
    else
      1800
    end_year <- if (corpus == "coha")
      2010
    else
      1990
    
    # Create lists of the group's available words
    
    # availwrds_decade_group1 <-
    #   lapply(1:length(wordvecs.dat), function(i) {
    #     groupwrds[, group1index][groupwrds[, group1index] != "" &
    #                                !groupwrds[, group1index] %in% unavwords[[i]]]
    #   })
    # 
    # availwrds_decade_group2 <-
    #   lapply(1:length(wordvecs.dat), function(i) {
    #     groupwrds[, group2index][groupwrds[, group2index] != "" &
    #                                !groupwrds[, group2index] %in% unavwords[[i]]]
    #   })
    
    availwrds_decade_group1 <-
      lapply(1:length(wordvecs.dat), function(i) {
        g1_words <- groupwrds[, group1index]
        g2_words <- groupwrds[, group2index]
        
        # Remove unavailable words for this decade
        valid_g1 <- g1_words != "" & !(g1_words %in% unavwords[[i]])
        valid_g2 <- g2_words != "" & !(g2_words %in% unavwords[[i]])
        
        if (parallelize) {
          # Only include where both group1 and group2 words at that index are valid
          valid <- valid_g1 & valid_g2
          g1_words[valid]
        } else {
          g1_words[valid_g1]
        }
      })
    
    availwrds_decade_group2 <-
      lapply(1:length(wordvecs.dat), function(i) {
        g1_words <- groupwrds[, group1index]
        g2_words <- groupwrds[, group2index]
        
        valid_g1 <- g1_words != "" & !(g1_words %in% unavwords[[i]])
        valid_g2 <- g2_words != "" & !(g2_words %in% unavwords[[i]])
        
        if (parallelize) {
          valid <- valid_g1 & valid_g2
          g2_words[valid]
        } else {
          g2_words[valid_g2]
        }
      })

    availwrds_decade_wordterms <-
      lapply(1:length(wordvecs.dat), function(i) {
        wordterms[!wordterms %in% unavwords[[i]]]
      })
    
    # Now compute MAC from available words for each decade
    wordvecs.mat <- list()
    mac_group1_2list <- list()
    cor_group1_2 <- list()
    cor_group1_2ts <- vector()
    for (i in 1:length(wordvecs.dat)) {
      wordvecs.mat[[i]] <- as.matrix(wordvecs.dat[[i]])
      
      mac_group1_2list[[i]] <-
        data.frame(
          grp1ef = mac(wordvecs.mat[[i]], S = wordterms, A = availwrds_decade_group1[[i]])$P,
          grp2ef = mac(wordvecs.mat[[i]], S = wordterms, A = availwrds_decade_group2[[i]])$P,
          trait = names(
            mac(wordvecs.mat[[i]], S = wordterms, A = availwrds_decade_group1[[i]])$P
          )
        )
      
      # requires 2 data points; occasionally early COHA doesn't have enough, so replace with NA
      cor_group1_2[[i]] <- tryCatch({
        cor.test(mac_group1_2list[[i]]$grp2ef, mac_group1_2list[[i]]$grp1ef)
      },
      error = function(e)
        NA)
      
      cor_group1_2ts[i] <- if (is.list(cor_group1_2[[i]])) {
        cor_group1_2[[i]]$estimate
      } else {
        NA
      }
      
      cor_group1_2ts <-
        ts(
          cor_group1_2ts,
          start = start_year,
          end = end_year,
          frequency = 1 / 10
        )
      print(i)
    }
    output_rel <- list(mac_group1_2list,
                       cor_group1_2ts)
    return(output_rel)
  }
```

An example of how the mac function works (using engall 1990); here we compute the mean average correlation of each word in the first list to the list of animals. It makes sense that the animals in the first list had the highest mac score.

```{r sanityCheck1}
print(mac(
  as.matrix(wordvecs.dat[[20]]),
  S = c("elephant", "horse", "tiger", "happy", "weird", "car"),
  A = c("dog", "cat", "turtle", "fish", "monkey") # the "group" words
)$P)
```

You can compute the cosine similarity of any two words by replacing the lists with single words.

```{r sanityCheck2}
print(mac(
  as.matrix(wordvecs.dat[[20]]),
  S = "happy",
  A = "sad"
)$P)
```
We can also see a MAC score for an association across decades; 1:5 indicates the first 5 decades available for engall.

```{r mac by decade}
lapply(1:5, function(i) mac(as.matrix(wordvecs.dat[[i]]), S = "teacher", A = "woman")$P)

```

Now, we will create some helper functions, starting with creating an empty dataframe to be filled in.

This custom function is built for our research question. It takes two sets of group words (e.g., man and woman words), a set of word terms (e.g., agency), and a corpus term (i.e., to define either engall or coha). It then calls the above workhorse function, computing the cosine similarities and then the higher-order correlations i.e., grpwrdassoc_rel

The get_data_internal function can be flexibly called for the whole time period or for a specific decade, converting into an easier-to-work-with dataframe.

The get_ts and get_decade functions are just simple-use wrappers for the above function. This likely helps to prevent mistakes.

```{r helperFunctions, echo = FALSE, message = FALSE}
overall_results <- data.frame()

generate_data <- function(group1index,
                          group2index,
                          wordterms,
                          corpus,
                          parallelize = FALSE) {
  file_path <- paste0(
    "outputs/",
    paste(
      group1index,
      group2index,
      wordterms,
      corpus,
      parallelize,
      sep = "_"
    ),
    ".RData"
  )
  
  if (file.exists(file_path)) {
    # Load the object from the .RData file into a temporary environment
    temp_env <- new.env()
    load(file_path, envir = temp_env)  # Load the object
    output <- temp_env$output  # Access the loaded object
  } else {
    if (corpus == "coha") {
      wordvecs.dat_selected <- wordvecs.dat_coha
      unavwords_selected <- unavwords_coha
    } else {
      wordvecs.dat_selected <- wordvecs.dat
      unavwords_selected <- unavwords
    }
    output <- grpwrdassoc_rel(
      group1index = group1index,
      group2index = group2index,
      wordterms = get(wordterms),
      wordvecs.dat = wordvecs.dat_selected,
      unavwords = unavwords_selected,
      corpus = corpus,
      parallelize = parallelize
    )
    
    if (!dir.exists("outputs")) {
      dir.create("outputs")
    }
    save(output, file = file_path)
  }
  
  return(output)
}

get_data_internal <- function(group1index,
                              group2index,
                              wordterms,
                              corpus,
                              decade = NULL,
                              parallelize = FALSE) {
  # Generate the data
  data <- generate_data(
    group1index = group1index,
    group2index = group2index,
    wordterms = wordterms,
    corpus = corpus,
    parallelize = parallelize
  )
  
  # Select the output based on the `decade`
  if (is.null(decade)) {
    output <- data[[2]]
    
    # Convert the output to a data frame
    years <- time(output)
    values <- as.numeric(output)
    
    new_entry <- data.frame(
      year = years,
      value = values,
      group1index = group1index,
      group2index = group2index,
      wordterms = wordterms,
      corpus = corpus,
      parallelize = parallelize
    )
    
    # Check if the combination already exists in the data frame
    existing <- overall_results %>%
      filter(
        group1index == !!group1index,
        group2index == !!group2index,
        wordterms == !!wordterms,
        corpus == !!corpus,
        parallelize == !!parallelize
      )
    
    # Append to the global results data frame only if the combination is new
    if (nrow(existing) == 0) {
      overall_results <<- bind_rows(overall_results, new_entry)
    }
  } else {
    if (corpus == "coha") {
      output <- data[[1]][[(decade - 1810) / 10]]
    } else {
      output <- data[[1]][[(decade - 1790) / 10]]
    }
    attr(output, "decade") <- decade
  }
  
  # Attach additional attributes
  attr(output, "group1index") <- group1index
  attr(output, "group2index") <- group2index
  attr(output, "wordterms") <- wordterms
  attr(output, "corpus") <- corpus
  attr(output, "parallelize") <- parallelize
  
  
  
  # Return the output regardless
  return(output)
}


get_ts <- function(group1index,
                   group2index,
                   wordterms,
                   corpus,
                   parallelize = FALSE) {
  get_data_internal(group1index, group2index, wordterms, corpus, parallelize=parallelize)
}

get_decade <-
  function(group1index,
           group2index,
           wordterms,
           corpus,
           decade,
           parallelize = FALSE) {
    get_data_internal(group1index,
                      group2index,
                      wordterms,
                      corpus,
                      decade = decade,
                      parallelize)
  }

```

Now, we test everything we have so far.

```{r sampleCalls, echo = FALSE}
# ts1 <- get_ts("men", "women", "agentic", "engall")
# ts2 <- get_ts("men", "women", "agentic", "coha")
# ts3 <- get_ts("nonhuman", "women", "noun", "coha")
ts2 <- get_ts("men", "women", "agentic", "coha", TRUE)




# decade1 <- get_decade("men", "women", "agentic", "engall", 1800)
# decade2 <- get_decade("men", "women", "agentic", "coha", 1820)
```

Now, we are creating functions for dynamic visualizations.

```{r makePlotsRevised, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)

plot_one_ts <-
  function(ts_data) {
    ts_df <- data.frame(Year = seq(from = 1800, to = 1990, by = 10),
                        Value = as.vector(ts_data))
    group1 <- attr(ts_data, "group1index")
    group2 <- attr(ts_data, "group2index")
    word_term <- attr(ts_data, "wordterms")
    corpus <- attr(ts_data, "corpus")
    title <-
      paste(group1, "vs", group2, ",", word_term, "(", corpus, ")")
    p <- ggplot(ts_df, aes(x = Year, y = Value)) +
      geom_line(color = "blue") +
      labs(title = title, x = "Year", y = "Value") +
      theme_minimal()
    
    return(p)
  }

plot_multiple_ts <- function(ts_list) {
  combined_df <- data.frame()
  for (i in seq_along(ts_list)) {
    ts_data <- ts_list[[i]]
    ts_df <- data.frame(
      Year = as.vector(time(ts_data)),
      Value = as.vector(ts_data),
      group1 = attr(ts_data, "group1index"),
      group2 = attr(ts_data, "group2index"),
      word_term = attr(ts_data, "wordterms"),
      corpus = attr(ts_data, "corpus")
    )
    combined_df <- rbind(combined_df, ts_df)
  }
  y_min <- min(min(combined_df$Value, na.rm = TRUE), 0)
  y_max <- max(max(combined_df$Value, na.rm = TRUE), 1)
  p <-
    ggplot(combined_df, aes(
      x = Year,
      y = Value,
      color = interaction(group1, group2, word_term)
    )) +
    geom_line() +
    labs(title = "Multiple Time Series Plot, Correlations", x = "Year", y = "Correlation Coefficient") +
    facet_wrap(~ corpus, scales = "fixed") +
    scale_y_continuous(limits = c(y_min, y_max)) +
    theme_minimal() + guides(color = guide_legend(title = NULL))
  
  return(p)
}

plot_one_decade <- function(decade_data) {
  # Extract attributes
  group1 <- attr(decade_data, "group1index")
  group2 <- attr(decade_data, "group2index")
  word_term <- attr(decade_data, "wordterms")
  corpus <- attr(decade_data, "corpus")
  decade <- attr(decade_data, "decade")
  
  # Calculate Pearson correlation
  pearson_corr <-
    cor(decade_data$grp1ef, decade_data$grp2ef, use = "complete.obs")
  
  # Create title
  title <- paste0(
    group1,
    " vs ",
    group2,
    ", ",
    word_term,
    " (",
    corpus,
    ", ",
    decade,
    ", r = ",
    round(pearson_corr, 2),
    ")"
  )
  
  # Create plot
  p <- ggplot(decade_data, aes(x = grp1ef, y = grp2ef)) +
    geom_point(color = "blue") + geom_smooth(method = "lm", formula = y ~
                                               x) +
    geom_text(aes(label = trait), vjust = -0.5, size = 3) +
    labs(title = title, x = group1, y = group2) +
    theme_minimal()
  
  return(p)
}

```

### Looking at a decade

We can plot the mac scores for two different groups against each other like so.

The correlation between men-agentic and women-agentic cosine similarities in coha in 1810. 


```{r plottingDecade, warning=FALSE}
plot_one_decade(get_decade("men", "women", "agentic", "coha", 1820))
plot_one_decade(get_decade("men", "women", "agentic", "engall", 1800))
plot_one_decade(get_decade("men", "women", "role", "engall", 1950))

```

The titles of the plots contain the Pearson coefficient, which is what we will use to measure the similarity of the two groups.

### Outlier analysis
Noticing that the 1820 coha plot had an odd correlation, let's check the proportion of gender words that were available, as this could be skewing the slope.
```{r propMissingCoha}
# Calculate the number of missing group words by decade
groupmiss_coha <-
  as.data.frame(sapply(1:dim(groupwrds)[2], function(j) {
    sapply(1:length(wordvecs.dat_coha), function(i) {
      sum(groupwrds[, j] %in% unavwords_coha[[i]])
    })
  }))
colnames(groupmiss_coha) <- colnames(groupwrds)
groupmiss_coha[21,] <- colSums(groupwrds != "", na.rm = TRUE)
groupmiss2_coha <-
  as.data.frame(sapply(1:dim(groupmiss_coha)[2], function(j) {
    1 - groupmiss_coha[1:20, j] / groupmiss_coha[21, j]
  }))
colnames(groupmiss2_coha) <- colnames(groupwrds)

# Add the year column
rownames(groupmiss2_coha) <- seq(1820, 2010, by = 10)
groupmiss2_coha$year <- seq(1820, 2010, by = 10)
groupmiss2_coha
```

```{r plotMissingCoha}
# Create the plot
ggplot(groupmiss2_coha, aes(x = year)) +
  geom_line(aes(y = men, color = "Men")) +
  geom_line(aes(y = women, color = "Women")) +
  scale_color_manual(values = c("Men" = "blue", "Women" = "red")) +
  labs(title = "prop. group words available over time (coha)",
       x = "Years",
       y = "Values",
       color = "Legend") +
  theme_minimal()
```

Clearly many fewer words were available in that first decade; let's check for statistical outliers.
```{r outliersMissingCoha}
find_outliers <- function(column, year) {
  is_outlier <-
    abs(column - mean(column, na.rm = TRUE)) > 3 * sd(column, na.rm = TRUE)
  data.frame(Decade = year[is_outlier], Value = column[is_outlier])
}

# Find outliers for men and women
outliers_men <-
  find_outliers(groupmiss2_coha$men, groupmiss2_coha$year)
outliers_women <-
  find_outliers(groupmiss2_coha$women, groupmiss2_coha$year)

# Print results
outliers_men
outliers_women
```
Repeat for engall:
```{r propMissingEngall}
# Calculate the number of missing group words by decade
groupmiss <-
  as.data.frame(sapply(1:dim(groupwrds)[2], function(j) {
    sapply(1:length(wordvecs.dat), function(i) {
      sum(groupwrds[, j] %in% unavwords[[i]])
    })
  }))
colnames(groupmiss) <- colnames(groupwrds)
groupmiss[21,] <- colSums(groupwrds != "", na.rm = TRUE)
groupmiss2 <-
  as.data.frame(sapply(1:dim(groupmiss)[2], function(j) {
    1 - groupmiss[1:20, j] / groupmiss[21, j]
  }))
colnames(groupmiss2) <- colnames(groupwrds)

# Add the year column
rownames(groupmiss2) <- seq(1800, 1990, by = 10)
groupmiss2$year <- seq(1800, 1990, by = 10)
groupmiss2
```

```{r plotMissingEngall}
# Create the plot
ggplot(groupmiss2, aes(x = year)) +
  geom_line(aes(y = men, color = "Men")) +
  geom_line(aes(y = women, color = "Women")) +
  scale_color_manual(values = c("Men" = "blue", "Women" = "red")) +
  labs(title = "prop. group words available over time (engall)",
       x = "Years",
       y = "Values",
       color = "Legend") +
  theme_minimal()
```
```{r outliersMissingEngall}
# Find outliers for men and women
outliers_men <-
  find_outliers(groupmiss2$men, groupmiss2_coha$year)
outliers_women <-
  find_outliers(groupmiss2$women, groupmiss2_coha$year)

# Print results
outliers_men
outliers_women
```
Engall has no outliers, as expected.

### Plots, engall

Now we can begin to plot the actual correlation values over time, starting with engall:
```{r plottingTsMenWomen}
men_women_trait_role_ts <-
  list(
    get_ts("men", "women", "trait", "engall"),
    get_ts("men", "women", "role", "engall")
  )
plot_multiple_ts(men_women_trait_role_ts)

men_women_agentic_communal_ts <-
  list(
    get_ts("men", "women", "agentic", "engall"),
    get_ts("men", "women", "communal", "engall")
  )
plot_multiple_ts(men_women_agentic_communal_ts)

```

Now, let's look at the actual magnitudes of the mac scores (rather than the Pearson correlations). To do this, we take the mean of all mac scores with a single group.
```{r meanFunc, echo=FALSE}
overall_results_averages <- data.frame()

get_ts_averages <- function(group1index,
                            wordterms,
                            corpus,
                            parallelize = FALSE) {
  if (corpus == "coha") {
    years <- seq(1820, 2010, by = 10)
  } else {
    years <- seq(1800, 1990, by = 10)
  }
  output <- sapply(years, function(year) {
    if (group1index == "men") {
      mean(
        get_decade(
          group1index,
          "women",
          wordterms,
          corpus,
          year,
          parallelize = parallelize
        )$grp1ef,
        na.rm = TRUE
      )
    } else if (group1index == "women") {
      mean(
        get_decade("men", "women", wordterms, corpus, year, parallelize = parallelize)$grp2ef,
        na.rm = TRUE
      )
    }
    else{
      mean(get_decade(group1index, "women", wordterms, corpus, year)$grp2ef,
           na.rm = TRUE)
    }
  })
  
  output <-
    ts(output,
       start = min(years),
       frequency = 1 / (years[2] - years[1]))
  
  years <- time(output)
  values <- as.numeric(output)
  
  new_entry <- data.frame(
    year = years,
    value = values,
    group1index = group1index,
    wordterms = wordterms,
    corpus = corpus
  )
  
  
  # Check if the combination already exists in the data frame
  existing <- overall_results_averages %>%
    filter(group1index == !!group1index,
           wordterms == !!wordterms,
           corpus == !!corpus)
  
  # Append to the global results data frame only if the combination is new
  if (nrow(existing) == 0) {
    overall_results_averages <<-
      bind_rows(overall_results_averages, new_entry)
  }
  
  
  attr(output, "group1index") <- group1index
  attr(output, "wordterms") <- wordterms
  attr(output, "corpus") <- corpus
  
  return(output)
}
```

```{r plottingMeans, echo = FALSE}
plot_multiple_ts_averages <- function(ts_list) {
  combined_df <- data.frame()
  for (i in seq_along(ts_list)) {
    ts_data <- ts_list[[i]]
    # print(as.vector(time(ts_data)))
    ts_df <- data.frame(
      Year = as.vector(time(ts_data)),
      Value = as.vector(ts_data),
      group1 = attr(ts_data, "group1index"),
      word_term = attr(ts_data, "wordterms"),
      corpus = attr(ts_data, "corpus")
    )
    combined_df <- rbind(combined_df, ts_df)
  }

  
  y_min <- min(min(combined_df$Value, na.rm = TRUE), -.1)
  y_max <- max(max(combined_df$Value, na.rm = TRUE), .1)
  
  p <-
    ggplot(combined_df, aes(
      x = Year,
      y = Value,
      color = interaction(group1, word_term)
    )) +
    geom_line() +
    labs(title = "Multiple Time Series Plot, Averages", x = "Year", y = "Correlation Coefficient") +
    facet_grid(corpus ~ word_term, scales = "fixed") +
    scale_y_continuous(limits = c(y_min, y_max)) +
    theme_minimal() + guides(color = guide_legend(title = NULL))
  
  return(p)
}
```

```{r plottingTsMenWomenAverages}
men_women_trait_role_ts <-
  list(
    get_ts_averages("men", "agentic", "engall"),
    get_ts_averages("men", "communal", "engall"),
    get_ts_averages("women", "agentic", "engall"),
    get_ts_averages("women", "communal", "engall")
  )
plot_multiple_ts_averages(men_women_trait_role_ts)
```

### Baseline with nonhuman groups

We can also do a baseline test with different groups to see if the men/women correlations are uniquely high.
```{r plottingTsHumanNonhuman, warning=FALSE}
human_nonhuman_ts <-
  list(
    get_ts("nonhuman", "women", "trait", "coha"),
    get_ts("nonhuman", "men", "trait", "coha"),
    get_ts("nonhuman", "women", "trait", "engall"),
    get_ts("nonhuman", "men", "trait", "engall"),
    get_ts("men", "women", "trait", "engall"),
    get_ts("men", "women", "trait", "coha")
  )
plot_multiple_ts(human_nonhuman_ts)

plot_one_decade(get_decade("nonhuman", "women", "trait", "engall", 1980))
plot_one_decade(get_decade("nonhuman", "men", "trait", "coha", 1980))


```

### Plots, coha
```{r plottingTsMenWomenCoha}
men_women_trait_role_ts <-
  list(
    get_ts("men", "women", "trait", "coha"),
    get_ts("men", "women", "role", "coha")
  )
plot_multiple_ts(men_women_trait_role_ts)

men_women_agentic_communal_ts <-
  list(
    get_ts("men", "women", "agentic", "coha"),
    get_ts("men", "women", "communal", "coha")
  )
plot_multiple_ts(men_women_agentic_communal_ts)

```

```{r plottingTsMenWomenAveragesCoha}
men_women_trait_role_ts <-
  list(
    get_ts_averages("men", "agentic", "coha"),
    get_ts_averages("men", "communal", "coha"),
    get_ts_averages("women", "agentic", "coha"),
    get_ts_averages("women", "communal", "coha")
  )
plot_multiple_ts_averages(men_women_trait_role_ts)
```

### Data in excel format
Access the files containing all the data, which can be filtered in excel:
```{r overallCSV}
head(overall_results)
write_xlsx(overall_results, "overall_results.xlsx")
```
[overall_results.xlsx](overall_results.xlsx)

```{r overallCSVaverages}
head(overall_results_averages)
write_xlsx(overall_results_averages, "overall_results_averages.xlsx")
```
[overall_results_averages.xlsx](overall_results_averages.xlsx)

### Plots, engall+coha
```{r extraStuff, warning=FALSE}
men_women_trait_role_ts <-
  list(
    get_ts("men", "women", "trait", "engall"),
    get_ts("men", "women", "role", "engall"),
    get_ts("men", "women", "trait", "coha"),
    get_ts("men", "women", "role", "coha")
  )
plot_multiple_ts(men_women_trait_role_ts)

men_women_agentic_communal_ts <-
  list(
    get_ts("men", "women", "agentic", "engall"),
    get_ts("men", "women", "communal", "engall"),
    get_ts("men", "women", "agentic", "coha"),
    get_ts("men", "women", "communal", "coha")
  )
plot_multiple_ts(men_women_agentic_communal_ts)

```

### Analysis of word count by gender

First, the raw counts can be viewed here:

[gender_word_counts.csv](gender_word_counts.csv)

[gender_word_counts_genre_aggregated.csv](gender_word_counts_genre_aggregated.csv)

The data was generated through a python script that collected counts from the original text. Here is a preview:

```{r combineByGenre, warning=FALSE, echo=FALSE, fig.show="hide"}
gender_word_counts <- read.csv("gender_word_counts.csv")
gender_word_counts_genre_aggregated <-
  read.csv("gender_word_counts_genre_aggregated.csv")

head(gender_word_counts)
head(gender_word_counts_genre_aggregated)
```

We also have access to the overall counts for each COHA decade in order to contextualize those for the gender words. They were acquired from https://www.english-corpora.org/coha/.

```{r overallCounts}
coha_counts <- read.csv("overall_counts.csv")

head(coha_counts)
```
Some have NA because genres TV and news weren't collected until later years.

We reshape it into tidy data like so:

```{r overallCountsShaped}
library(tidyr)
library(tidyverse)

genre_map <-
  c(
    "NON.FICTIONBOOKS" = "acad",
    "FICTION" = "fic",
    "POPULARMAGAZINES" = "mag",
    "NEWSPAPERS" = "news",
    "TV.MOVIES" = "tvm"
  )

coha_counts_tidy <- coha_counts %>%
  slice(1:(n() - 1)) %>%
  pivot_longer(
    cols = names(coha_counts)[-c(1, (ncol(coha_counts) - 2):ncol(coha_counts))],
    names_to = "genre",
    values_to = "count"
  ) %>%
  mutate(decade = as.integer(str_remove(DECADE, "s$")),
         genre = recode(genre,!!!genre_map)) %>%
  select(decade, genre, count)


coha_counts_tidy

coha_counts_genre_aggregated <-
  coha_counts_tidy %>%  group_by(decade) %>% summarise(count = sum(count, na.rm = TRUE))

coha_counts_genre_aggregated
```
Now we can find the relative frequency of each term within a decade.

```{r genderFrequency}
gender_word_frequencies <-
  gender_word_counts_genre_aggregated %>% merge(coha_counts_genre_aggregated,
                                                by.x = "decade",
                                                by.y = "decade") %>% mutate(prop = count.x / count.y) %>% select(decade, word, gender, prop)

head(gender_word_frequencies)
```

We can also aggregate the words by gender:

```{r freqByGender}
gender_frequencies <-
  gender_word_frequencies %>% group_by(gender, decade) %>% summarize(prop =
                                                                       sum(prop))

head(gender_frequencies)

gender_frequencies %>% ggplot(aes(x = decade, y = prop, color = gender)) +
  geom_point()+ggtitle("Representation of gender in COHA")

```
An analysis by genre, just for fun:

```{r freqByGenderAndGenre}
gender_word_frequencies_by_genre <-
  gender_word_counts %>% merge(coha_counts_tidy,
                                                by.x = c("decade", "genre"),
                                                by.y = c("decade","genre")) %>% mutate(prop = count.x / count.y) %>% select(decade, genre, word, gender, prop)

head(gender_word_frequencies_by_genre)
```

Meaning, in 1820, "men" represented about 1.15e-3 of the acad genre.

```{r plotByGenre}
gender_frequencies_by_genre <-
  gender_word_frequencies_by_genre %>% group_by(decade, gender, genre) %>% summarise(prop =
                                                                                       sum(prop))

head(gender_frequencies_by_genre)

gender_frequencies_by_genre %>% ggplot(aes(x = decade, y = prop, color = gender)) +
  geom_point() + facet_wrap(vars(genre)) + ggtitle("Representation of COHA genres by gender")
```
We can try and see which words are making the most contribution.

```{r freqByWord}
gender_word_frequencies

gender_word_frequencies_cumulative_proportion <- 
  gender_word_frequencies %>%
  group_by(decade, gender) %>%
  arrange(desc(prop), .by_group = TRUE) %>%
  mutate(
    total_prop = sum(prop),
    cum_prop_of_group = cumsum(prop) / total_prop
  ) %>%
  ungroup() %>% select(decade, word, gender, prop, cum_prop_of_group)

gender_word_frequencies_cumulative_proportion%>%filter(gender=="female")

gender_word_frequencies_cumulative_proportion %>%
  filter(decade %% 70 == 20) %>%
  ggplot(aes(
    x = reorder(word, cum_prop_of_group),
    y = cum_prop_of_group,
  )) +
  geom_line(aes(group = gender)) +
  facet_grid(decade ~ gender, scales = "free_x", switch = "x") +
  labs(
    title = "Cumulative Proportion of Word Contribution to a Decade",
    x = "Words (Ordered by Proportion Within Group)",
    y = "Cumulative Proportion of Group",
    color = "Gender"
  ) +
  theme_minimal() +
  theme(
    strip.placement = "outside",
    panel.spacing = unit(1, "lines"),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  ) +
  # Add axis text only for bottom row
  theme(axis.text.x.bottom = element_text(
    angle = 90,
    vjust = 0.5,
    hjust = 1
  ))

```
Clearly, a small number of words is making a large contribution to each gender group.


### Analyses, repeated but with parallelized lists

We repeat everything as before, but by first parallelizing the group lists. This means taking the lists to be only words for which the male and female complement were available as vectors for a given decade.

An optional input variable, parallelize, was added to the utility functions that get the time series. We will re-plot everything and set parallelize to true.

```{r plottingTsMenWomenParallel}
men_women_trait_role_ts <-
  list(
    get_ts("men", "women", "trait", "engall", TRUE),
    get_ts("men", "women", "role", "engall", TRUE),
    get_ts("men", "women", "trait", "coha", TRUE),
    get_ts("men", "women", "role", "coha", TRUE)
  )
plot_multiple_ts(men_women_trait_role_ts)

men_women_agentic_communal_ts <-
  list(
    get_ts("men", "women", "agentic", "engall", TRUE),
    get_ts("men", "women", "communal", "engall", TRUE),
    get_ts("men", "women", "agentic", "coha", TRUE),
    get_ts("men", "women", "communal", "coha", TRUE)
  )
plot_multiple_ts(men_women_agentic_communal_ts)

men_women_trait_role_ts <-
  list(
    get_ts_averages("men", "agentic", "coha", TRUE),
    get_ts_averages("men", "communal", "coha", TRUE),
    get_ts_averages("women", "agentic", "coha", TRUE),
    get_ts_averages("women", "communal", "coha", TRUE)
  )
plot_multiple_ts_averages(men_women_trait_role_ts)

men_women_trait_role_ts <-
  list(
    get_ts_averages("men", "agentic", "engall", TRUE),
    get_ts_averages("men", "communal", "engall", TRUE),
    get_ts_averages("women", "agentic", "engall", TRUE),
    get_ts_averages("women", "communal", "engall", TRUE)
  )
plot_multiple_ts_averages(men_women_trait_role_ts)

```