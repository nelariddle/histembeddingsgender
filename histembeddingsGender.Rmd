---
title: "Tran_Riddle_Historical Embeddings"
output: 
  html_document:
    toc: true
    toc_float: true
  github_document:
date: December 3, 2024
author: Nela Riddle
---
[Visit the original repository on GitHub](https://github.com/nelariddle/histembeddingsgender)

We load in the required packages.

```{r loadPackages, echo = FALSE, message=FALSE, warning=FALSE}
########################################################

## What are the historical patterns of social group representations?
## 14 social groups, 200 years of Google Books Text

########################################################

## Set up R workspace and packages ----
## Set WD to source file location (python codes, stored data, scripts, etc. can all be shared to one folder for ease of use)
# setwd("**")

# load packages
if (!require("corrplot")) {install.packages("corrplot", dependencies = TRUE);require(corrplot)}
if (!require("reticulate")) {install.packages("reticulate", dependencies = TRUE);require(reticulate)}
if (!require("lsa")) {install.packages("lsa", dependencies = TRUE);require(lsa)}
if (!require("dplyr")) {install.packages("dplyr", dependencies = TRUE);require(dplyr)}
if (!require("devtools")) {install.packages("devtools", dependencies = TRUE);require(devtools)}
# if (!require("sweater")) {devtools::install_github("chainsawriot/sweater");require(sweater)} 
if (!require("sweater")) {install.packages("sweater");require(sweater)} 
if (!require("writexl")) {install.packages("writexl");require(writexl)} 
# contains convenience functions for embeddings analyses 
# see: https://rdrr.io/github/chainsawriot/sweater/f/README.md
```

### Setup
First, load in the pre-written group and word lists to be used in analyses. We are creating a custom function for reading in the word lists. It assumes no headers and single vector of words.

Then, we pull in the group words and trait/role words files file and doing some basic cleaning.

```{r loadWords, include=TRUE, warning=FALSE}
## Load in data ----
## Set WD to word stimuli
setwd("wordstim")

# Function to read and process lists
read_list <- function(file, col_name) {
  list_data <- read.delim(file, header = FALSE)
  colnames(list_data) <- col_name
  return(as.vector(list_data[[col_name]]))
}

# Specific groups (men, women)
groupwrds <- read.csv("groupstimlists.csv", header = FALSE)
groupwrds <- as.data.frame(t(groupwrds))
colnames(groupwrds) <- as.character(groupwrds[1, ])
groupwrds <- groupwrds[-1, ]

# Read lists using the function
agentic <- read_list("agentic.txt", "agentic")
communal <- read_list("communal.txt", "communal")
trait <- read_list("traitlist.txt", "trait")
chore <- read_list("chore.txt", "chore")
role <- read_list("joblistDOT.txt", "role")
role2 <-
  sub("s$", "", tolower(
    read.csv(
      "All_Occupations.csv",
      header = TRUE,
      stringsAsFactors = FALSE
    )$Occupation[!grepl(
      "\\s",
      read.csv(
        "All_Occupations.csv",
        header = TRUE,
        stringsAsFactors = FALSE
      )$Occupation
    )]
  ))
role <- union(role, role2)
role <- union(role, chore)


noun <- read_list("nouns.txt", "noun")
common <- read_list("common.txt", "common")

setwd("..")
```
The agentic and communal lists were borrowed from https://onlinelibrary.wiley.com/doi/10.1002/ejsp.2561; here are some examples:
```{r wordExamples}
head(agentic)
head(communal)
```

The group word lists were taken from https://pubmed.ncbi.nlm.nih.gov/35787033/, as well as the trait list:
```{r groupExamples}
head(groupwrds$men)
head(groupwrds$women)
head(trait)
```
The role titles were scraped off this site: https://theodora.com/dot_index.html, a 1971 survey on role titles. They were merged with one-word titles from ONET, the modern equivalent: https://www.onetonline.org/find/all. They were merged with the chore list to represent unpaid labor.
```{r roleExamples, echo=FALSE, message=FALSE}
head(role)
```

Now, we define which words are not available in both sets of embeddings across time.

This was a decision made by the original folks who trained these embeddings (histwords) "Furthermore, the authors only computed embeddings for words that were among the top 100,000 most frequent words over all time (for EngAll) or top 50,000 most frequent words (for COHA), and further discarded any words that occurred less than 500 times (for EngAll) or 100 times (for COHA) in a given decade."

```{r loadVectors, echo=FALSE, message=FALSE}
# Check unavailable words by decade ----
if (!exists("wordvecs.dat", envir = .GlobalEnv)) {
  load("engall/wordvecsdata_engall.RData", envir = .GlobalEnv)
}
if (!exists("wordvecs.dat_coha", envir = .GlobalEnv)) {
  load("coha/wordvecsdata_coha.RData", envir = .GlobalEnv)
}
unavwords <- lapply(wordvecs.dat, function(x)
  rownames(x)[which(x$V1 == 0)])
n_avwords <- sapply(wordvecs.dat, function(x)
  length(x$V1) - length(rownames(x)[which(x$V1 == 0)]))

unavwords_coha <- lapply(wordvecs.dat_coha, function(x)
  rownames(x)[which(x$V1 == 0)])
n_avwords_coha <- sapply(wordvecs.dat_coha, function(x)
  length(x$V1) - length(rownames(x)[which(x$V1 == 0)])
)
```
### Producing the mac scores

The workhorse function, which iterates over each decade, computing the MAC (mean average cosine similarity) score between each word and each group, then finds the Pearson correlation of the resulting lists (demonstrated visually later).

```{r importantFunction, message=FALSE}
grpwrdassoc_rel <-
  function(group1index,
           group2index,
           wordterms,
           wordvecs.dat = wordvecs.dat,
           unavwords = unavwords,
           corpus) {
    start_year <- if (corpus == "coha")
      1810
    else
      1800
    end_year <- if (corpus == "coha")
      2000
    else
      1990
    
    # Create lists of the group's available words
    availwrds_decade_group1 <-
      lapply(1:length(wordvecs.dat), function(i) {
        groupwrds[, group1index][groupwrds[, group1index] != "" &
                                   !groupwrds[, group1index] %in% unavwords[[i]]]
      })
    
    availwrds_decade_group2 <-
      lapply(1:length(wordvecs.dat), function(i) {
        groupwrds[, group2index][groupwrds[, group2index] != "" &
                                   !groupwrds[, group2index] %in% unavwords[[i]]]
      })
    
    availwrds_decade_wordterms <-
      lapply(1:length(wordvecs.dat), function(i) {
        wordterms[!wordterms %in% unavwords[[i]]]
      })
    
    # Now compute MAC from available words for each decade
    wordvecs.mat <- list()
    mac_group1_2list <- list()
    cor_group1_2 <- list()
    cor_group1_2ts <- vector()
    for (i in 1:length(wordvecs.dat)) {
      wordvecs.mat[[i]] <- as.matrix(wordvecs.dat[[i]])
      
      mac_group1_2list[[i]] <-
        data.frame(
          grp1ef = mac(wordvecs.mat[[i]], S = wordterms, A = availwrds_decade_group1[[i]])$P,
          grp2ef = mac(wordvecs.mat[[i]], S = wordterms, A = availwrds_decade_group2[[i]])$P,
          trait = names(
            mac(wordvecs.mat[[i]], S = wordterms, A = availwrds_decade_group1[[i]])$P
          )
        )
      
      # requires 2 data points; occasionally early COHA doesn't have enough, so replace with NA
      cor_group1_2[[i]] <- tryCatch({
        cor.test(mac_group1_2list[[i]]$grp2ef, mac_group1_2list[[i]]$grp1ef)
      },
      error = function(e)
        NA)
      
      cor_group1_2ts[i] <- if (is.list(cor_group1_2[[i]])) {
        cor_group1_2[[i]]$estimate
      } else {
        NA
      }
      
      cor_group1_2ts <-
        ts(
          cor_group1_2ts,
          start = start_year,
          end = end_year,
          frequency = 1 / 10
        )
      print(i)
    }
    output_rel <- list(mac_group1_2list,
                       cor_group1_2ts)
    return(output_rel)
  }
```

An example of how the mac function works (using engall 1990); here we compute the mean average correlation of each word in the first list to the list of animals. It makes sense that the animals in the first list had the highest mac score.

```{r sanityCheck1}
print(mac(
  as.matrix(wordvecs.dat[[20]]),
  S = c("elephant", "horse", "tiger", "happy", "weird", "car"),
  A = c("dog", "cat", "turtle", "fish", "monkey") # the "group" words
)$P)
```

You can compute the cosine similarity of any two words by replacing the lists with single words.

```{r sanityCheck2}
print(mac(
  as.matrix(wordvecs.dat[[20]]),
  S = "happy",
  A = "sad"
)$P)
```
We can also see a MAC score for an association across decades.

```{r mac by decade}
for (i in 1:20) {
  print(mac(as.matrix(wordvecs.dat[[i]]),
            S = "teacher",
            A = "woman")$P)
}
```

Now, we will create some helper functions, starting with creating an empty dataframe to be filled in.

This custom function is built for our research question. It takes two sets of group words (e.g., man and woman words), a set of word terms (e.g., agency), and a corpus term (i.e., to define either engall or coha). It then calls the above workhorse function, computing the cosine similarities and then the higher-order correlations i.e., grpwrdassoc_rel

The get_data_internal function  can be flexibly called for the whole time period or for a specific decade, converting into an easier-to-work-with dataframe.

The get_ts and get_decade functions are just simple-use wrappers for the above function. This likely helps to prevent mistakes.

```{r helperFunctions, echo = FALSE, message = FALSE}
overall_results <- data.frame()

generate_data <- function(group1index,
                          group2index,
                          wordterms,
                          corpus) {
  file_path <- paste0("outputs/",
                      paste(group1index, group2index, wordterms, corpus, sep = "_"),
                      ".RData")
  if (file.exists(file_path)) {
    # Load the object from the .RData file into a temporary environment
    temp_env <- new.env()
    load(file_path, envir = temp_env)  # Load the object
    output <- temp_env$output  # Access the loaded object
  } else {
    if (corpus == "coha") {
      wordvecs.dat_selected <- wordvecs.dat_coha
      unavwords_selected <- unavwords_coha
    } else {
      wordvecs.dat_selected <- wordvecs.dat
      unavwords_selected <- unavwords
    }
    output <- grpwrdassoc_rel(
      group1index = group1index,
      group2index = group2index,
      wordterms = get(wordterms),
      wordvecs.dat = wordvecs.dat_selected,
      unavwords = unavwords_selected,
      corpus = corpus
    )
    
    if (!dir.exists("outputs")) {
      dir.create("outputs")
    }
    save(output, file = file_path)
  }
  
  return(output)
}

get_data_internal <- function(group1index,
                              group2index,
                              wordterms,
                              corpus,
                              decade = NULL) {
  # Generate the data
  data <- generate_data(
    group1index = group1index,
    group2index = group2index,
    wordterms = wordterms,
    corpus = corpus
  )
  
  # Select the output based on the `decade`
  if (is.null(decade)) {
    output <- data[[2]]
    
    # Convert the output to a data frame
    years <- time(output)
    values <- as.numeric(output)
    
    new_entry <- data.frame(
      year = years,
      value = values,
      group1index = group1index,
      group2index = group2index,
      wordterms = wordterms,
      corpus = corpus
    )
    
    
    # Check if the combination already exists in the data frame
    existing <- overall_results %>%
      filter(
        group1index == !!group1index,
        group2index == !!group2index,
        wordterms == !!wordterms,
        corpus == !!corpus
      )
    
    # Append to the global results data frame only if the combination is new
    if (nrow(existing) == 0) {
      overall_results <<- bind_rows(overall_results, new_entry)
    }
  } else {
    if (corpus == "coha") {
      output <- data[[1]][[(decade - 1800) / 10]]
    } else {
      output <- data[[1]][[(decade - 1790) / 10]]
    }
    attr(output, "decade") <- decade
  }
  
  # Attach additional attributes
  attr(output, "group1index") <- group1index
  attr(output, "group2index") <- group2index
  attr(output, "wordterms") <- wordterms
  attr(output, "corpus") <- corpus
  
  
  # Return the output regardless
  return(output)
}


get_ts <- function(group1index,
                   group2index,
                   wordterms,
                   corpus) {
  get_data_internal(group1index, group2index, wordterms, corpus)
}

get_decade <-
  function(group1index,
           group2index,
           wordterms,
           corpus,
           decade) {
    get_data_internal(group1index, group2index, wordterms, corpus, decade = decade)
  }

```

Now, we test everything we have so far.

```{r sampleCalls, echo = FALSE}
ts1 <- get_ts("men", "women", "agentic", "engall")
ts2 <- get_ts("men", "women", "communal", "coha")
ts3 <- get_ts("men", "women", "noun", "coha")
ts4 <- get_ts("nonhuman", "women", "noun", "coha")



decade1 <- get_decade("men", "women", "agentic", "engall", 1800)
decade2 <- get_decade("men", "women", "agentic", "coha", 1810)
decade2 <- get_decade("men", "women", "noun", "coha", 2000)
```
Now, we are creating functions for dynamic visualizations.

```{r makePlotsRevised, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)

plot_one_ts <-
  function(ts_data) {
    ts_df <- data.frame(Year = seq(from = 1800, to = 1990, by = 10),
                        Value = as.vector(ts_data))
    group1 <- attr(ts_data, "group1index")
    group2 <- attr(ts_data, "group2index")
    word_term <- attr(ts_data, "wordterms")
    corpus <- attr(ts_data, "corpus")
    title <-
      paste(group1, "vs", group2, ",", word_term, "(", corpus, ")")
    p <- ggplot(ts_df, aes(x = Year, y = Value)) +
      geom_line(color = "blue") +
      labs(title = title, x = "Year", y = "Value") +
      theme_minimal()
    
    return(p)
  }

plot_multiple_ts <- function(ts_list) {
  combined_df <- data.frame()
  for (i in seq_along(ts_list)) {
    ts_data <- ts_list[[i]]
    ts_df <- data.frame(
      Year = as.vector(time(ts_data)),
      Value = as.vector(ts_data),
      group1 = attr(ts_data, "group1index"),
      group2 = attr(ts_data, "group2index"),
      word_term = attr(ts_data, "wordterms"),
      corpus = attr(ts_data, "corpus")
    )
    combined_df <- rbind(combined_df, ts_df)
  }
  y_min <- min(min(combined_df$Value, na.rm = TRUE), 0)
  y_max <- max(max(combined_df$Value, na.rm = TRUE), 1)
  p <-
    ggplot(combined_df, aes(
      x = Year,
      y = Value,
      color = interaction(group1, group2, word_term)
    )) +
    geom_line() +
    labs(title = "Multiple Time Series Plot, Correlations", x = "Year", y = "Correlation Coefficient") +
    facet_wrap( ~ corpus, scales = "fixed") +
    scale_y_continuous(limits = c(y_min, y_max)) +
    theme_minimal() + guides(color = guide_legend(title = NULL))
  
  return(p)
}

plot_one_decade <- function(decade_data) {
  # Extract attributes
  group1 <- attr(decade_data, "group1index")
  group2 <- attr(decade_data, "group2index")
  word_term <- attr(decade_data, "wordterms")
  corpus <- attr(decade_data, "corpus")
  decade <- attr(decade_data, "decade")
  
  # Calculate Pearson correlation
  pearson_corr <-
    cor(decade_data$grp1ef, decade_data$grp2ef, use = "complete.obs")
  
  # Create title
  title <- paste0(
    group1,
    " vs ",
    group2,
    ", ",
    word_term,
    " (",
    corpus,
    ", ",
    decade,
    ", r = ",
    round(pearson_corr, 2),
    ")"
  )
  
  # Create plot
  p <- ggplot(decade_data, aes(x = grp1ef, y = grp2ef)) +
    geom_point(color = "blue") + geom_smooth(method = "lm", formula = y ~
                                               x) +
    geom_text(aes(label = trait), vjust = -0.5, size = 3) +
    labs(title = title, x = group1, y = group2) +
    theme_minimal()
  
  return(p)
}

```

### Looking at a decade

We can plot the mac scores for two different groups against each other like so.

The correlation between men-agentic and women-agentic cosine similarities in coha in 1810. 


```{r plottingDecade, warning=FALSE}
plot_one_decade(get_decade("men", "women", "agentic", "coha", 1810))
plot_one_decade(get_decade("men", "women", "agentic", "engall", 1800))
plot_one_decade(get_decade("men", "women", "role", "engall", 1950))

```

The titles of the plots contain the Pearson coefficient, which is what we will use to measure the similarity of the two groups.

### Outlier analysis
Noticing that the 1810 coha plot had an odd correlation, let's check the proportion of gender words that were available, as this could be skewing the slope.
```{r propMissingCoha}
# Calculate the number of missing group words by decade
groupmiss_coha <-
  as.data.frame(sapply(1:dim(groupwrds)[2], function(j) {
    sapply(1:length(wordvecs.dat_coha), function(i) {
      sum(groupwrds[, j] %in% unavwords_coha[[i]])
    })
  }))
colnames(groupmiss_coha) <- colnames(groupwrds)
groupmiss_coha[21,] <- colSums(groupwrds != "", na.rm = TRUE)
groupmiss2_coha <-
  as.data.frame(sapply(1:dim(groupmiss_coha)[2], function(j) {
    1 - groupmiss_coha[1:20, j] / groupmiss_coha[21, j]
  }))
colnames(groupmiss2_coha) <- colnames(groupwrds)

# Add the year column
rownames(groupmiss2_coha) <- seq(1810, 2000, by = 10)
groupmiss2_coha$year <- seq(1810, 2000, by = 10)
groupmiss2_coha
```

```{r plotMissingCoha}
# Create the plot
ggplot(groupmiss2_coha, aes(x = year)) +
  geom_line(aes(y = men, color = "Men")) +
  geom_line(aes(y = women, color = "Women")) +
  scale_color_manual(values = c("Men" = "blue", "Women" = "red")) +
  labs(title = "prop. group words available over time (coha)",
       x = "Years",
       y = "Values",
       color = "Legend") +
  theme_minimal()
```

Clearly many fewer words were available in that first decade; let's check for statistical outliers.
```{r outliersMissingCoha}
find_outliers <- function(column, year) {
  is_outlier <-
    abs(column - mean(column, na.rm = TRUE)) > 3 * sd(column, na.rm = TRUE)
  data.frame(Decade = year[is_outlier], Value = column[is_outlier])
}

# Find outliers for men and women
outliers_men <-
  find_outliers(groupmiss2_coha$men, groupmiss2_coha$year)
outliers_women <-
  find_outliers(groupmiss2_coha$women, groupmiss2_coha$year)

# Print results
outliers_men
outliers_women
```
Repeat for engall:
```{r propMissingEngall}
# Calculate the number of missing group words by decade
groupmiss <-
  as.data.frame(sapply(1:dim(groupwrds)[2], function(j) {
    sapply(1:length(wordvecs.dat), function(i) {
      sum(groupwrds[, j] %in% unavwords[[i]])
    })
  }))
colnames(groupmiss) <- colnames(groupwrds)
groupmiss[21,] <- colSums(groupwrds != "", na.rm = TRUE)
groupmiss2 <-
  as.data.frame(sapply(1:dim(groupmiss)[2], function(j) {
    1 - groupmiss[1:20, j] / groupmiss[21, j]
  }))
colnames(groupmiss2) <- colnames(groupwrds)

# Add the year column
rownames(groupmiss2) <- seq(1800, 1990, by = 10)
groupmiss2$year <- seq(1800, 1990, by = 10)
groupmiss2
```

```{r plotMissingEngall}
# Create the plot
ggplot(groupmiss2, aes(x = year)) +
  geom_line(aes(y = men, color = "Men")) +
  geom_line(aes(y = women, color = "Women")) +
  scale_color_manual(values = c("Men" = "blue", "Women" = "red")) +
  labs(title = "prop. group words available over time (engall)",
       x = "Years",
       y = "Values",
       color = "Legend") +
  theme_minimal()
```
```{r outliersMissingEngall}
# Find outliers for men and women
outliers_men <-
  find_outliers(groupmiss2$men, groupmiss2_coha$year)
outliers_women <-
  find_outliers(groupmiss2$women, groupmiss2_coha$year)

# Print results
outliers_men
outliers_women
```
Engall has no outliers, as expected.

### Plots, engall

Now we can begin to plot the actual correlation values over time, starting with engall:
```{r plottingTsMenWomen}
men_women_trait_role_ts <-
  list(
    get_ts("men", "women", "trait", "engall"),
    get_ts("men", "women", "role", "engall")
  )
plot_multiple_ts(men_women_trait_role_ts)

men_women_agentic_communal_ts <-
  list(
    get_ts("men", "women", "agentic", "engall"),
    get_ts("men", "women", "communal", "engall")
  )
plot_multiple_ts(men_women_agentic_communal_ts)

```

Now, let's look at the actual magnitudes of the mac scores (rather than the Pearson correlations). To do this, we take the mean of all mac scores with a single group.
```{r meanFunc, echo=FALSE}
overall_results_averages<-data.frame()

get_ts_averages <- function(group1index,
                            wordterms,
                            corpus) {
  if (corpus == "coha") {
    years <- seq(1810, 2000, by = 10)
  } else {
    years <- seq(1800, 1990, by = 10)
  }
  output <- sapply(years, function(year) {
    mean(get_decade(group1index, "women", wordterms, corpus, year)$grp1ef,
         na.rm = TRUE)
  })
  
  output <-
    ts(output,
       start = min(years),
       frequency = 1 / (years[2] - years[1]))
  
  years <- time(output)
  values <- as.numeric(output)
  
  new_entry <- data.frame(
    year = years,
    value = values,
    group1index = group1index,
    wordterms = wordterms,
    corpus = corpus
  )
  
  
  # Check if the combination already exists in the data frame
  existing <- overall_results_averages %>%
    filter(group1index == !!group1index,
           wordterms == !!wordterms,
           corpus == !!corpus)
  
  # Append to the global results data frame only if the combination is new
  if (nrow(existing) == 0) {
    overall_results_averages <<- bind_rows(overall_results_averages, new_entry)
  }
  
  
  attr(output, "group1index") <- group1index
  attr(output, "wordterms") <- wordterms
  attr(output, "corpus") <- corpus
  
  return(output)
}
```

```{r plottingMeans, echo = FALSE}
plot_multiple_ts_averages <- function(ts_list) {
  combined_df <- data.frame()
  for (i in seq_along(ts_list)) {
    ts_data <- ts_list[[i]]
    # print(as.vector(time(ts_data)))
    ts_df <- data.frame(
      Year = as.vector(time(ts_data)),
      Value = as.vector(ts_data),
      group1 = attr(ts_data, "group1index"),
      word_term = attr(ts_data, "wordterms"),
      corpus = attr(ts_data, "corpus")
    )
    combined_df <- rbind(combined_df, ts_df)
  }

  
  y_min <- min(min(combined_df$Value, na.rm = TRUE), -.1)
  y_max <- max(max(combined_df$Value, na.rm = TRUE), .1)
  
  p <-
    ggplot(combined_df, aes(
      x = Year,
      y = Value,
      color = interaction(group1, word_term)
    )) +
    geom_line() +
    labs(title = "Multiple Time Series Plot, Averages", x = "Year", y = "Correlation Coefficient") +
    facet_grid(corpus ~ word_term, scales = "fixed") +
    scale_y_continuous(limits = c(y_min, y_max)) +
    theme_minimal() + guides(color = guide_legend(title = NULL))
  
  return(p)
}
```

```{r plottingTsMenWomenAverages}
men_women_trait_role_ts <-
  list(
    get_ts_averages("men", "agentic", "engall"),
    get_ts_averages("men", "communal", "engall"),
    get_ts_averages("women", "agentic", "engall"),
    get_ts_averages("women", "communal", "engall")
  )
plot_multiple_ts_averages(men_women_trait_role_ts)
```

### Baseline with nonhuman groups

We can also do a baseline test with different groups to see if the men/women correlations are uniquely high.
```{r plottingTsHumanNonhuman, warning=FALSE}
human_nonhuman_ts <-
  list(
    get_ts("nonhuman", "women", "trait", "coha"),
    get_ts("nonhuman", "men", "trait", "coha"),
    get_ts("nonhuman", "women", "trait", "engall"),
    get_ts("nonhuman", "men", "trait", "engall"),
    get_ts("men", "women", "trait", "engall"),
    get_ts("men", "women", "trait", "coha")
  )
plot_multiple_ts(human_nonhuman_ts)

plot_one_decade(get_decade("nonhuman", "women", "trait", "engall", 1980))
plot_one_decade(get_decade("nonhuman", "men", "trait", "coha", 1980))


```

### Plots, coha
```{r plottingTsMenWomenCoha}
men_women_trait_role_ts <-
  list(
    get_ts("men", "women", "trait", "coha"),
    get_ts("men", "women", "role", "coha")
  )
plot_multiple_ts(men_women_trait_role_ts)

men_women_agentic_communal_ts <-
  list(
    get_ts("men", "women", "agentic", "coha"),
    get_ts("men", "women", "communal", "coha")
  )
plot_multiple_ts(men_women_agentic_communal_ts)

```

```{r plottingTsMenWomenAveragesCoha}
men_women_trait_role_ts <-
  list(
    get_ts_averages("men", "agentic", "coha"),
    get_ts_averages("men", "communal", "coha"),
    get_ts_averages("women", "agentic", "coha"),
    get_ts_averages("women", "communal", "coha")
  )
plot_multiple_ts_averages(men_women_trait_role_ts)
```

### Data in excel format
Access the files containing all the data, which can be filtered in excel:
```{r overallCSV}
head(overall_results)
write_xlsx(overall_results, "overall_results.xlsx")
```
[overall_results.xlsx](overall_results.xlsx)

```{r overallCSVaverages}
head(overall_results_averages)
write_xlsx(overall_results_averages, "overall_results_averages.xlsx")
```
[overall_results_averages.xlsx](overall_results_averages.xlsx)

### Plots, engall+coha
```{r extraStuff, warning=FALSE}
men_women_trait_role_ts <-
  list(
    get_ts("men", "women", "trait", "engall"),
    get_ts("men", "women", "role", "engall"),
    get_ts("men", "women", "trait", "coha"),
    get_ts("men", "women", "role", "coha")
  )
plot_multiple_ts(men_women_trait_role_ts)

men_women_agentic_communal_ts <-
  list(
    get_ts("men", "women", "agentic", "engall"),
    get_ts("men", "women", "communal", "engall"),
    get_ts("men", "women", "agentic", "coha"),
    get_ts("men", "women", "communal", "coha")
  )
plot_multiple_ts(men_women_agentic_communal_ts)

```

### Analysis of word count by gender

First, the raw counts can be viewed here:

[gender_word_counts.csv](gender_word_counts.csv)

[gender_word_counts_genre_aggregated.csv](gender_word_counts_genre_aggregated.csv)

```{r combineByGenre, warning=FALSE, echo=FALSE, fig.show="hide"}
library(dplyr)

word_freq <- read.csv("gender_word_counts.csv")

head(word_freq)
```

```{r visualizingWordCounts, warning=FALSE, echo=FALSE, fig.show="hide"}
# Load tidyverse
library(tidyverse)

# Read the CSV file
gender_counts <- read_csv("gender_word_counts.csv")

# Summarize total word counts
df_summary <- gender_counts %>%
  group_by(gender, word) %>%
  summarise(total_count = sum(count), .groups = 'drop') %>%
  arrange(desc(total_count))

# Get top 15 words per gender
top_words <- df_summary %>%
  group_by(gender) %>%
  slice_max(order_by = total_count, n = 15)

# Create static bar plot
ggplot(top_words, aes(x = reorder(word, total_count), y = total_count, fill = gender)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  facet_wrap(~gender, scales = "free_y") +
  labs(title = "Top Gendered Words by Frequency",
       x = "Word", y = "Total Count") +
  theme_minimal()



```

```{r plot3, warning=FALSE, echo=FALSE, fig.show="hide"}
# Load required libraries
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(scales)

# Read the dataset
gender_word_counts <- read_csv("gender_word_counts.csv")

# Define word pair lists
male_words <- c(
  "men", "man", "male", "males", "masculine", "masculinity",
  "he", "him", "his", "himself",
  "mr", "mister",
  "boy", "boys",
  "guy", "guys",
  "fella", "fellas",
  "gent", "gents",
  "sir", "sirs",
  "lad", "lads",
  "gentleman", "gentlemen",
  "prince", "princes",
  "king", "kings"
)

female_words <- c(
  "women", "woman", "female", "females", "feminine", "femininity",
  "she", "her", "hers", "herself",
  "mrs", "ms",
  "girl", "girls",
  "gal", "gals",
  "lass", "lassie",
  "dame", "dames",
  "madam", "miss",
  "maiden", "maidens",
  "gentlewoman", "gentlewomen",
  "princess", "princesses",
  "queen", "queens"
)

# Build a lookup table of word -> word_pair_group
word_pair_groups <- data.frame(
  male_word = male_words,
  female_word = female_words,
  word_pair_group = paste0("pair_", seq_along(male_words))
) %>%
  pivot_longer(cols = c(male_word, female_word),
               names_to = "original_gender",
               values_to = "word") %>%
  select(word, word_pair_group)

# Merge and keep only gendered words
gendered_data_grouped <- gender_word_counts %>%
  inner_join(word_pair_groups, by = "word")

# Define the cumulative proportion threshold (e.g., 99% for top 99% of word counts)
cumulative_threshold <- 0.5  # Change this value as needed (e.g., 0.90 for 90%)

# Filter to top words by cumulative proportion
top_gendered_words <- gendered_data_grouped %>%
  group_by(decade, gender) %>%
  mutate(total_word_count = sum(count),
         word_proportion = count / total_word_count) %>%
  arrange(decade, gender, desc(word_proportion)) %>%
  mutate(cumulative_proportion = cumsum(word_proportion)) %>%
  filter(cumulative_proportion <= cumulative_threshold) %>%
  ungroup()

# Flip female counts for stacking below center (for visualization)
top_gendered_words <- top_gendered_words %>%
  mutate(display_count = ifelse(gender == "female", -count, count))

# Order word_pair_group by total count so biggest stacks are closest to center
ordered_pairs <- top_gendered_words %>%
  group_by(word_pair_group) %>%
  summarise(total = sum(abs(display_count)), .groups = "drop") %>%
  arrange(desc(total)) %>%
  pull(word_pair_group)

top_gendered_words <- top_gendered_words %>%
  mutate(word_pair_group = factor(word_pair_group, levels = ordered_pairs))

# Create pretty labels like "he/she"
pair_labels <- setNames(
  paste0(male_words, "/", female_words),
  paste0("pair_", seq_along(male_words))
)

# Filter out any word pair groups that aren't present in the plot (those with no count)
top_gendered_words <- top_gendered_words %>%
  filter(word_pair_group %in% unique(top_gendered_words$word_pair_group))

# Create plot
ggplot(top_gendered_words, aes(x = factor(decade), y = display_count, fill = word_pair_group)) +
  geom_bar(stat = "identity", position = "stack") +
  labs(
    title = "Gendered Word Usage Over Time",
    subtitle = paste0("Top words contributing to ", cumulative_threshold * 100, "% of word counts by decade and gender"),
    x = "Decade",
    y = "Word Count (Female values plotted negative)",
    fill = "Word Pair"
  ) +
  scale_y_continuous(labels = abs) +
  scale_fill_manual(
    values = hue_pal()(length(pair_labels)),
    labels = pair_labels[unique(top_gendered_words$word_pair_group)]
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```